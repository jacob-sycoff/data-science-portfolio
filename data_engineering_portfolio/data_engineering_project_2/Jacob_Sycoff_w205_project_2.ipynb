{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w205 Project 2\n",
    "### Jacob Sycoff\n",
    "#### Due: 10/29/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"assessment-attempts-20180128-121051-nested.json\",\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments= json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_exam_id': '4beeac16-bb83-4d58-83e4-26cdc38f0481',\n",
      " 'certification': 'false',\n",
      " 'exam_name': 'The Principles of Microservices',\n",
      " 'keen_created_at': '1516738973.653394',\n",
      " 'keen_id': '5a67999d3ed3e300016ef0f1',\n",
      " 'keen_timestamp': '1516738973.653394',\n",
      " 'max_attempts': '1.0',\n",
      " 'sequences': {'attempt': 1,\n",
      "               'counts': {'all_correct': False,\n",
      "                          'correct': 3,\n",
      "                          'incomplete': 0,\n",
      "                          'incorrect': 1,\n",
      "                          'submitted': 4,\n",
      "                          'total': 4,\n",
      "                          'unanswered': 0},\n",
      "               'id': 'b370a3aa-bf9e-4c10-848a-8ecacbd1d93e',\n",
      "               'questions': [{'id': 'b9ff2e88-cf9d-4bd4-bcea-c2673779425c',\n",
      "                              'options': [{'at': '2018-01-23T20:22:39.089Z',\n",
      "                                           'checked': True,\n",
      "                                           'id': 'c315723f-4753-4f8f-932e-d9d7085fed28',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T20:22:27.250Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'f298bc63-9075-45bc-8b95-9c8c979bf779',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': '302fa72e-3949-46a1-9dd9-668fac9c1fc1'}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'incorrect',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'bec23e7b-4870-49f7-92c1-1e0a016ef7c9',\n",
      "                              'options': [{'at': '2018-01-23T20:22:41.102Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '60b45b0b-f20c-42eb-8b26-61ef690fd1ba',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'id': 'a5a583b6-9c3a-4147-afbb-d6fbfc516c85'},\n",
      "                                          {'at': '2018-01-23T20:22:42.988Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'a3e31a57-6407-40f1-9808-c7f2ed8e7f95',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T20:22:44.447Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'aae4e562-49bf-46ca-808a-5300ac4b08d1',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': '1ba75b31-64a4-4bd3-9646-25aa3b73505f',\n",
      "                              'options': [{'checked': False,\n",
      "                                           'id': 'ac290741-c86c-491a-9872-72213e947a70'},\n",
      "                                          {'at': '2018-01-23T20:22:46.048Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'a9d65a6d-0fdf-4518-b1a0-900d9248f123',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': '1f7c5def-904b-4834-8519-639a3b22a496',\n",
      "                              'options': [{'checked': False,\n",
      "                                           'id': 'cb8e8ecb-5a24-4712-ae46-81bc5b858c3b'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '429b03ec-bda9-4d4a-bd7f-e2d208cbb5fc'},\n",
      "                                          {'at': '2018-01-23T20:22:50.811Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '1cdc931d-dbfb-4aea-bcb8-57be503bb649',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T20:22:52.423Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '088b406e-1fa6-419f-8193-d41c29ee4d57',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True}]},\n",
      " 'started_at': '2018-01-23T20:22:22.584Z',\n",
      " 'user_exam_id': '8edbc8a8-4d26-4292-a5af-ae3f246cb09f'}\n"
     ]
    }
   ],
   "source": [
    "# this will pretty print the json in alphabetic order which may or may not match the file order\n",
    "p.pprint(assessments[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursive_walk_json_object(j, level):\n",
    "    \"\"\"recursively walk through a json object to explore the structure\n",
    "       dictionaries will be put in alphabetic order to match the pretty print above\"\"\"\n",
    "    \n",
    "    level += 1\n",
    "    \n",
    "    if type(j) is dict:\n",
    "        dict_2_list = list(j.keys())\n",
    "        dict_2_list.sort()\n",
    "        for k in dict_2_list:\n",
    "            print(\"   \" * level + \"L\" + str(level), k)\n",
    "            recursive_walk_json_object(j[k], level)\n",
    "    \n",
    "    elif type(j) is list:\n",
    "        for (i, l) in enumerate(j):\n",
    "            print(\"  \" * level + \"  [\" + str(i) + \"]\")\n",
    "            recursive_walk_json_object(l, level)\n",
    "            \n",
    "    else:\n",
    "        print(\"   \" * level + \" value:\", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 base_exam_id\n",
      "    value: 37f0a30a-7464-11e6-aa92-a8667f27e5dc\n",
      "L0 certification\n",
      "    value: false\n",
      "L0 exam_name\n",
      "    value: Normal Forms and All That Jazz Master Class\n",
      "L0 keen_created_at\n",
      "    value: 1516717442.735266\n",
      "L0 keen_id\n",
      "    value: 5a6745820eb8ab00016be1f1\n",
      "L0 keen_timestamp\n",
      "    value: 1516717442.735266\n",
      "L0 max_attempts\n",
      "    value: 1.0\n",
      "L0 sequences\n",
      "   L1 attempt\n",
      "       value: 1\n",
      "   L1 counts\n",
      "      L2 all_correct\n",
      "          value: False\n",
      "      L2 correct\n",
      "          value: 2\n",
      "      L2 incomplete\n",
      "          value: 1\n",
      "      L2 incorrect\n",
      "          value: 1\n",
      "      L2 submitted\n",
      "          value: 4\n",
      "      L2 total\n",
      "          value: 4\n",
      "      L2 unanswered\n",
      "          value: 0\n",
      "   L1 id\n",
      "       value: 5b28a462-7a3b-42e0-b508-09f3906d1703\n",
      "   L1 questions\n",
      "      [0]\n",
      "         L3 id\n",
      "             value: 7a2ed6d3-f492-49b3-b8aa-d080a8aad986\n",
      "         L3 options\n",
      "          [0]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:24.670Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: 49c574b4-5c82-4ffd-9bd1-c3358faf850d\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "          [1]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:25.914Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: f2528210-35c3-4320-acf3-9056567ea19f\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "          [2]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: d1bf026f-554f-4543-bdd2-54dcf105b826\n",
      "         L3 user_correct\n",
      "             value: False\n",
      "         L3 user_incomplete\n",
      "             value: True\n",
      "         L3 user_result\n",
      "             value: missed_some\n",
      "         L3 user_submitted\n",
      "             value: True\n",
      "      [1]\n",
      "         L3 id\n",
      "             value: bbed4358-999d-4462-9596-bad5173a6ecb\n",
      "         L3 options\n",
      "          [0]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:30.116Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: a35d0e80-8c49-415d-b8cb-c21a02627e2b\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "          [1]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\n",
      "          [2]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:41.791Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: 7e0b639a-2ef8-4604-b7eb-5018bd81a91b\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "         L3 user_correct\n",
      "             value: False\n",
      "         L3 user_incomplete\n",
      "             value: False\n",
      "         L3 user_result\n",
      "             value: incorrect\n",
      "         L3 user_submitted\n",
      "             value: True\n",
      "      [2]\n",
      "         L3 id\n",
      "             value: e6ad8644-96b1-4617-b37b-a263dded202c\n",
      "         L3 options\n",
      "          [0]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:52.510Z\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 id\n",
      "                   value: a9333679-de9d-41ff-bb3d-b239d6b95732\n",
      "          [1]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 id\n",
      "                   value: 85795acc-b4b1-4510-bd6e-41648a3553c9\n",
      "          [2]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:54.223Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: c185ecdb-48fb-4edb-ae4e-0204ac7a0909\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "          [3]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:23:53.862Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: 77a66c83-d001-45cd-9a5a-6bba8eb7389e\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "         L3 user_correct\n",
      "             value: True\n",
      "         L3 user_incomplete\n",
      "             value: False\n",
      "         L3 user_result\n",
      "             value: correct\n",
      "         L3 user_submitted\n",
      "             value: True\n",
      "      [3]\n",
      "         L3 id\n",
      "             value: 95194331-ac43-454e-83de-ea8913067055\n",
      "         L3 options\n",
      "          [0]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 id\n",
      "                   value: 59b9fc4b-f239-4850-b1f9-912d1fd3ca13\n",
      "          [1]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 id\n",
      "                   value: 2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\n",
      "          [2]\n",
      "               L5 checked\n",
      "                   value: False\n",
      "               L5 id\n",
      "                   value: 62feee6e-9b76-4123-bd9e-c0b35126b1f1\n",
      "          [3]\n",
      "               L5 at\n",
      "                   value: 2018-01-23T14:24:00.807Z\n",
      "               L5 checked\n",
      "                   value: True\n",
      "               L5 correct\n",
      "                   value: True\n",
      "               L5 id\n",
      "                   value: 7f13df9c-fcbe-4424-914f-2206f106765c\n",
      "               L5 submitted\n",
      "                   value: 1\n",
      "         L3 user_correct\n",
      "             value: True\n",
      "         L3 user_incomplete\n",
      "             value: False\n",
      "         L3 user_result\n",
      "             value: correct\n",
      "         L3 user_submitted\n",
      "             value: True\n",
      "L0 started_at\n",
      "    value: 2018-01-23T14:23:19.082Z\n",
      "L0 user_exam_id\n",
      "    value: 6d4089e4-bde5-4a22-b65f-18bce9ab79c8\n"
     ]
    }
   ],
   "source": [
    "recursive_walk_json_object(assessments[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='assessments', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='ct', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='questions', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='sequences', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='ut', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "#i added this cell\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='assessments', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='ct', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='questions', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='sequences', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='ut', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "#i added this cell\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_id from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------------------+\n",
      "|    keen_timestamp|sequences[questions] AS `questions`[0][user_incomplete]|\n",
      "+------------------+-------------------------------------------------------+\n",
      "| 1516717442.735266|                                                   true|\n",
      "| 1516717377.639827|                                                  false|\n",
      "| 1516738973.653394|                                                  false|\n",
      "|1516738921.1137421|                                                  false|\n",
      "| 1516737000.212122|                                                  false|\n",
      "| 1516740790.309757|                                                  false|\n",
      "|1516746279.3801291|                                                  false|\n",
      "| 1516743820.305464|                                                  false|\n",
      "|  1516743098.56811|                                                  false|\n",
      "| 1516743764.813107|                                                  false|\n",
      "+------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "    return Row(**my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.registerTempTable('sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        sequences_id|\n",
      "+--------------------+\n",
      "|5b28a462-7a3b-42e...|\n",
      "|5b28a462-7a3b-42e...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|04a192c1-4f5c-4ac...|\n",
      "|e7110aed-0d08-4cb...|\n",
      "|5251db24-2a6e-424...|\n",
      "|066b5326-e547-4da...|\n",
      "|8ac691f8-8c1a-403...|\n",
      "|066b5326-e547-4da...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequences_id from sequences limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+------------------------------------+\n",
      "|keen_id                 |keen_timestamp    |sequences_id                        |\n",
      "+------------------------+------------------+------------------------------------+\n",
      "|5a17a67efa125700015e3164|1511499390.3836269|8ac691f8-8c1a-4033-b2e2-44e165775992|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|9bd87823-4508-4e09-a376-8b81bd40f207|\n",
      "|5a29dcac74b6620001da2dcf|1512692908.8423469|e7110aed-0d08-4cb3-9eca-3eb7caad0256|\n",
      "|5a2fdab0eabeda0001ad6d97|1513085616.2275269|cd800e92-afc3-4473-a6aa-6cad1aac6be1|\n",
      "|5a30105020e9d400013db76c|1513099344.8624721|8ac691f8-8c1a-4033-b2e2-44e165775992|\n",
      "|5a3a6fc3f0a10000019bf958|1513779139.354213 |e7110aed-0d08-4cb3-9eca-3eb7caad0256|\n",
      "|5a4e17fe08a8920001743e5f|1515067390.1336551|9abd5b51-6bd8-11e6-9f64-a8667f27e5dc|\n",
      "|5a4f3c69cc6444000130dd2d|1515142249.858722 |083844c5-772f-48df-9a41-8cffd27dcae0|\n",
      "|5a51b21bd0480b000126ab02|1515303451.773272 |e7110aed-0d08-4cb3-9eca-3eb7caad0256|\n",
      "|5a575a85329e1a00014113a9|1515674245.348099 |25ca21fe-4dbb-446c-9592-aa6c252c001f|\n",
      "+------------------------+------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from assessments a join sequences s on a.keen_id = s.keen_id limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions.registerTempTable('questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------+\n",
      "|id                                  |my_count|\n",
      "+------------------------------------+--------+\n",
      "|7a2ed6d3-f492-49b3-b8aa-d080a8aad986|1       |\n",
      "|bbed4358-999d-4462-9596-bad5173a6ecb|2       |\n",
      "|e6ad8644-96b1-4617-b37b-a263dded202c|3       |\n",
      "|95194331-ac43-454e-83de-ea8913067055|4       |\n",
      "|95194331-ac43-454e-83de-ea8913067055|1       |\n",
      "|bbed4358-999d-4462-9596-bad5173a6ecb|2       |\n",
      "|e6ad8644-96b1-4617-b37b-a263dded202c|3       |\n",
      "|7a2ed6d3-f492-49b3-b8aa-d080a8aad986|4       |\n",
      "|b9ff2e88-cf9d-4bd4-bcea-c2673779425c|1       |\n",
      "|bec23e7b-4870-49f7-92c1-1e0a016ef7c9|2       |\n",
      "+------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, my_count from questions limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+------------------------------------+\n",
      "|keen_id                 |keen_timestamp    |id                                  |\n",
      "+------------------------+------------------+------------------------------------+\n",
      "|5a17a67efa125700015e3164|1511499390.3836269|803fc93f-7eb2-4121-af8c-ff809e850f10|\n",
      "|5a17a67efa125700015e3164|1511499390.3836269|f3cb88cc-5b79-41bb-96d3-7ec7d698b964|\n",
      "|5a17a67efa125700015e3164|1511499390.3836269|32fe7d8d-6d89-4db4-a17a-a368c5ea3ca0|\n",
      "|5a17a67efa125700015e3164|1511499390.3836269|5c34cf19-8cfd-4f56-91c2-0a109dc990b9|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|0603e6f4-c3f9-4c23-8cb3-eb43c68d9113|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|26a06b88-2758-45bc-93c5-5e48deebb2ba|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|25b6effe-79b0-4c47-83d0-2b5cfd655dd3|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|6de03a9b-2a78-46b6-9c47-284477b57f28|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|aaf39991-fa83-470f-b0f0-c548ef3a2eba|\n",
      "|5a26ee9cbf5ce1000131c8e9|1512500892.4166169|aab2e817-73dc-4ff9-8d8b-a258a26e949f|\n",
      "+------------------------+------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_correct_total.registerTempTable('ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|correct|total|\n",
      "+-------+-----+\n",
      "|      2|    4|\n",
      "|      1|    4|\n",
      "|      3|    4|\n",
      "|      2|    4|\n",
      "|      3|    4|\n",
      "|      5|    5|\n",
      "|      1|    1|\n",
      "|      5|    5|\n",
      "|      4|    4|\n",
      "|      0|    5|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|score|\n",
      "+-----+\n",
      "|  0.5|\n",
      "| 0.25|\n",
      "| 0.75|\n",
      "|  0.5|\n",
      "| 0.75|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select correct / total as score from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My code starts here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Included Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "version: '2'\n",
      "services:\n",
      "  zookeeper:\n",
      "    image: confluentinc/cp-zookeeper:latest\n",
      "    environment:\n",
      "      ZOOKEEPER_CLIENT_PORT: 32181\n",
      "      ZOOKEEPER_TICK_TIME: 2000\n",
      "    expose:\n",
      "      - \"2181\"\n",
      "      - \"2888\"\n",
      "      - \"32181\"\n",
      "      - \"3888\"\n",
      "\n",
      "  kafka:\n",
      "    image: confluentinc/cp-kafka:latest\n",
      "    depends_on:\n",
      "      - zookeeper\n",
      "    environment:\n",
      "      KAFKA_BROKER_ID: 1\n",
      "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
      "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
      "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
      "    expose:\n",
      "      - \"9092\"\n",
      "      - \"29092\"\n",
      "\n",
      "  cloudera:\n",
      "    image: midsw205/cdh-minimal:latest\n",
      "    expose:\n",
      "      - \"8020\" # nn\n",
      "      - \"50070\" # nn http\n",
      "      - \"8888\" # hue\n",
      "    #ports:\n",
      "    #- \"8888:8888\"\n",
      "\n",
      "  spark:\n",
      "    image: midsw205/spark-python:0.0.5\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "      - ~/w205:/w205\n",
      "    expose:\n",
      "      - \"8888\"\n",
      "    ports:\n",
      "      - \"8888:8888\"\n",
      "    command: bash\n",
      "    depends_on:\n",
      "      - cloudera\n",
      "    environment:\n",
      "      HADOOP_NAMENODE: cloudera\n",
      "\n",
      "  mids:\n",
      "    image: midsw205/base:latest\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "      - ~/w205:/w205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc = open('docker-compose.yml')\n",
    "print(dc.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1  ls -la\n",
      "    2  ls\n",
      "    3  docker pull midsw205/base\n",
      "    4  mkdir ~/w205\n",
      "    5  docker run -it --rm -v ~/w205:/w205 midsw205/base:latest bash\n",
      "    6  cd ~/w205\n",
      "    7  ls -l\n",
      "    8  git clone https://github.com/mids-w205-crook/course-content.git\n",
      "    9  ls -lk\n",
      "   10  ls -l\n",
      "   11  cd course-content/\n",
      "   12  ls\n",
      "   13  ls -l\n",
      "   14  02-Working-with-Data/\n",
      "   15  cd 02-Working-with-Data/\n",
      "   16  ls\n",
      "   17  open async-videos.md \n",
      "   18  xdg-open async-videos.md \n",
      "   19  sudo apt xdg\n",
      "   20  sudo apt xdg-open\n",
      "   21  cd ~\n",
      "   22  cd /w205\n",
      "   23  ls\n",
      "   24  cd w205\n",
      "   25  pwd\n",
      "   26  ls -l\n",
      "   27  git clone https://github.com/mids-w205-crook/signup-jacob-sycoff.git\n",
      "   28  ls -l\n",
      "   29  cd\n",
      "   30  cd w205\n",
      "   31  ls\n",
      "   32  cd signup-jacob-sycoff/\n",
      "   33  git status\n",
      "   34  git branch assignment\n",
      "   35  git status\n",
      "   36  git checkout assignment\n",
      "   37  git status\n",
      "   38  vi README.md \n",
      "   39  VI README.MD\n",
      "   40  vi README.md \n",
      "   41  nano README.md \n",
      "   42  vi README.md \n",
      "   43  git status\n",
      "   44  git add README.md \n",
      "   45  git status\n",
      "   46  git commit -m\"i made a random edit\"\n",
      "   47  git config --global user.email \"jsycoff@berkeley.edu\"\n",
      "   48  git config --global user.name \"Jacob Sycoff\"\n",
      "   49  git commit -m\"i made a random edit\"\n",
      "   50  git status\n",
      "   51  git push origin assignment\n",
      "   52  ls\n",
      "   53  cd tutorials\\\n",
      "   54  ls\n",
      "   55  cd ai-platform\n",
      "   56  ls\n",
      "   57  xdg-open ai-explanations-image.ipynb\n",
      "   58  open ai-explanations-image.ipynb \n",
      "   59  ai-explanations-image.ipynb\n",
      "   60  cd ai-explanations-image.ipynb\n",
      "   61  xldg-open\n",
      "   62  xdlg-open\n",
      "   63  cd ~\n",
      "   64  pwd\n",
      "   65  clear\n",
      "   66  ipython\n",
      "   67  jupyter notebook\n",
      "   68  ls -la\n",
      "   69  cd w205\n",
      "   70  ls -la\n",
      "   71  git clone https://github.com/mids-w205-crook/project-1-jacob-sycoff.git\n",
      "   72  ls -la\n",
      "   73  cd project-1-jacob-sycoff\n",
      "   74  ls -la\n",
      "   75  open example.ipynb\n",
      "   76  clear\n",
      "   77  docker pull midsw205/base\n",
      "   78  ls -la\n",
      "   79  pwd\n",
      "   80  cd ..\n",
      "   81  curl -L -o annot_fpid.json https://goo.gl/qWiu7d\n",
      "   82  curl -L -o lp_data.csv https://goo.gl/FDFPYB\n",
      "   83  jq\n",
      "   84  head lp_data.csv\n",
      "   85  tail lp_data.csv\n",
      "   86  head -n1 lp_data.csv\n",
      "   87  cat lp_data.csv | wc -l\n",
      "   88  clear\n",
      "   89  clear\n",
      "   90  ls -la\n",
      "   91  un docker run -it --rm -v ~/w205:/w205 midsw205/base:latest bas\n",
      "   92  docker run -it --rm -v ~/w205:/w205 midsw205/base:latest bash\n",
      "   93  pwd\n",
      "   94  cd ~w205\n",
      "   95  cd ~/w205\n",
      "   96  ls -la\n",
      "   97  cat lp_data.csv | wc -l\n",
      "   98  cat lp_data.csv | sort\n",
      "   99  man sort\n",
      "  100  cat lp_data.csv | sort -g\n",
      "  101  cat lp_data.csv | sort -n\n",
      "  102  head annot_fpid.json\n",
      "  103  cat annot_fpid.json | jq .\n",
      "  104  cat annot_fpid.json | jq '.[][]'\n",
      "  105  cat annot_fpid.json | jq '.[][]' -r\n",
      "  106  cat annot_fpid.json | jq '.[][]' -r | sort \n",
      "  107  cat annot_fpid.json | jq '.[][]' -r | sort | uniq \n",
      "  108  cat annot_fpid.json | jq '.[][]' -r | sort | uniq -c \n",
      "  109  cat annot_fpid.json | jq '.[][]' -r | sort | uniq -c | sort -g\n",
      "  110  cat annot_fpid.json | jq '.[][]' -r | sort | uniq -c | sort -gr\n",
      "  111  cat annot_fpid.json | jq '.[][]' -r | sort | uniq -c | sort -gr | head -10\n",
      "  112  bq\n",
      "  113  bq query --use_legacy_sql=false 'SELECT count(*) FROM `bigquery-public-data.san_francisco.bikeshare_status`'\n",
      "  114  bq query --use_legacy_sql=false 'SELECT count(distinct station_id) FROM `bigquery-public-data.san_francisco.bikeshare_status`'\n",
      "  115  pwd\n",
      "  116  ls\n",
      "  117  cd w205\n",
      "  118  ls\n",
      "  119  cd project-1-jacob-sycoff\n",
      "  120  ls\n",
      "  121  open example.ipynb\n",
      "  122  jupyter open example.ipynb\n",
      "  123  git pull project-1-jacob-sycoff\n",
      "  124  cd ..\n",
      "  125  git pull project-1-jacob-sycoff\n",
      "  126  cd ..\n",
      "  127  ls\n",
      "  128  git pull w205\n",
      "  129  git status\n",
      "  130  cd w205\n",
      "  131  git status\n",
      "  132  cd project-1-jacob-sycoff\n",
      "  133  git status\n",
      "  134  git pull\n",
      "  135  ls\n",
      "  136  xdg-open readme.md\n",
      "  137  open d\n",
      "  138  README.md\n",
      "  139  vi README.md\n",
      "  140  ls\n",
      "  141  cd w205\n",
      "  142  ls -la\n",
      "  143  cd signup-jacob-sycoff\n",
      "  144  ls -la\n",
      "  145  git branch assignment\n",
      "  146  git checkout assignment\n",
      "  147  ls\n",
      "  148  ls -la\n",
      "  149  git add project-1-jacob-sycoff.ipynb\n",
      "  150  cd w203\n",
      "  151  la\n",
      "  152  ls -la\n",
      "  153  cd w205\n",
      "  154  cd project-1-jacob-sycoff/\n",
      "  155  ls -la\n",
      "  156  git add project-1-jacob-sycoff\n",
      "  157  git add project-1-jacob-sycoff.ipynb \n",
      "  158  git commit -m \"I created this file and am adding it to github. I will commit it often\"\n",
      "  159  git push origin assignment\n",
      "  160  checkout assignment\n",
      "  161  git checkout assignment\n",
      "  162  git status\n",
      "  163  cd ..\n",
      "  164  get checkout assignment\n",
      "  165  git checkout assignment\n",
      "  166  ls -la\n",
      "  167  cd project-1-jacob-sycoff/\n",
      "  168  git checkout assignment\n",
      "  169  git branch assignment\n",
      "  170  git checkout assignment\n",
      "  171  git add project-1-jacob-sycoff.ipynb \n",
      "  172  git commit -m \"Trying this again. Hopefully this works and will be my first commit. Commit often!\"\n",
      "  173  git add project-1-jacob-sycoff.ipynb \n",
      "  174  git commit -m \"Trying this again. Hopefully this works and will be my first commit. Commit often!\"\n",
      "  175  git status\n",
      "  176  git push origin assignment\n",
      "  177  git add project-1-jacob-sycoff.ipynb \n",
      "  178  git commit -m \"I did questions 1 and 2 of part 1\"\n",
      "  179  git push origin assignment\n",
      "  180  docker ps\n",
      "  181  docker images\n",
      "  182  docker ps -a\n",
      "  183  docker rm -f reverent driscoll\n",
      "  184  docker rm -f reverent_driscoll\n",
      "  185  docker run -it --rm -v ~/w205:/w205 midsw205/base pwd\n",
      "  186  docker ps -a\n",
      "  187  docker ps\n",
      "  188  docker run -it --rm -v ~/w205:/w205 midsw205/base:latest bash\n",
      "  189  docker run -it -v ~/w205:/w205 midsw205/base:latest bash\n",
      "  190  docker network -ls\n",
      "  191  docker network ls\n",
      "  192  docker network prune\n",
      "  193  docker pull midsw205/base:latest\n",
      "  194  docker pull midsw205/base:0.1.8\n",
      "  195  docker pull midsw205/base:0.1.9\n",
      "  196  docker pull redis\n",
      "  197  docker pull confluentinc/cp-zookeeper:latest\n",
      "  198  docker pull confluentinc/cp-kafka:latest\n",
      "  199  docker pull midsw205/spark-python:0.0.5\n",
      "  200  docker pull midsw205/spark-python:0.0.6\n",
      "  201  docker pull midsw205/cdh-minimal:latest\n",
      "  202  docker pull midsw205/hadoop:0.0.2\n",
      "  203  docker pull midsw205/presto:0.0.1\n",
      "  204  pwd\n",
      "  205  cd w205\n",
      "  206  ls\n",
      "  207  cd project-1-jacob-sycoff/\n",
      "  208  ls\n",
      "  209  git add project-1-jacob-sycoff.ipynb \n",
      "  210  git commit -m \" finished part one of the project\"\n",
      "  211  git status\n",
      "  212  git origin assignment\n",
      "  213  git push origin assignment\n",
      "  214  git add project-1-jacob-sycoff.ipynb \n",
      "  215  git commit -m \"Missed a part in section 1 - will be done soon\"\n",
      "  216  git push origin assignment\n",
      "  217  git add project-1-jacob-sycoff.ipynb \n",
      "  218  git commit -m \"Section one of project 1 actually finished now\"\n",
      "  219  git push origin assignment\n",
      "  220  cd ..\n",
      "  221  clear\n",
      "  222  bq query --use_legacy_sql=false 'SELECT\n",
      "  223  bq query --use_legacy_sql=false '\n",
      "  224  select count(*)\n",
      "  225  FROM bike_trip_data.bikeshare_tri[s\n",
      "  226  bq query --use_legacy_sql=false '\n",
      "  227  select count(*)\n",
      "  228  FROM bike_trip_data.bikeshare_trips'\n",
      "  229  bq query --use_legacy_sql=false '\n",
      "  230  SELECT COUNT(*)\n",
      "  231  FROM bike_trip_data.bikeshare_trips'\n",
      "  232  docker run -it --rm -v ~/w205:/w205 midsw205/base:latest bash\n",
      "  233  docker ps\n",
      "  234  docker ps -a\n",
      "  235  docker ps\n",
      "  236  docker ps -a\n",
      "  237  docker psd\n",
      "  238  docker ps\n",
      "  239  docker ps -a\n",
      "  240  docker ps\n",
      "  241  docker ps -a\n",
      "  242  docker rm -f beautiful_kalen\n",
      "  243  dcoker ps -f beautiful_kalam\n",
      "  244  docker rm -f beautiful_kalam\n",
      "  245  docker ps -a\n",
      "  246  man docker rm\n",
      "  247  docker ps -a\n",
      "  248  docker images\n",
      "  249  docker run -it --rm -v ~/w205:/w205 midsw205/base pwd\n",
      "  250  ls -la\n",
      "  251  cd w205\n",
      "  252  ls -la\n",
      "  253  cd project-1-jacob-sycoff/\n",
      "  254  ls\n",
      "  255  git add project-1-jacob-sycoff.ipynb \n",
      "  256  git commit -m \"For part 1 of the project, I reformatted the aql to make it look nice\"\n",
      "  257  git push origin assignment\n",
      "  258  ls\n",
      "  259  ls -la\n",
      "  260  cd w205\n",
      "  261  ls -la\n",
      "  262  project-1-jacob-sycoff/\n",
      "  263  cd project-1-jacob-sycoff/\n",
      "  264  ls -la\n",
      "  265  git checkout assignment\n",
      "  266  bq query --use_legacy_sql=false'\n",
      "  267          SELECT COUNT(*)\n",
      "  268          FROM `bike_trip_data.bikeshare_trips` '\n",
      "  269  bq query --use_legacy_sql=false'\n",
      "  270  >         SELECT COUNT(*)\n",
      "  271  >         FROM `bike_trip_data.bikeshare_trips` '\n",
      "  272  bq query --use_legacy_sql=false '\n",
      "  273  >         SELECT COUNT(*)\n",
      "  274  >         FROM `bike_trip_data.bikeshare_trips` '\n",
      "  275  bq query --use_legacy_sql=false '\n",
      "  276  SELECT COUNT(*)\n",
      "  277  FROM `bike_trip_data.bikeshare_trips` '\n",
      "  278  bq query --use_legacy_sql=false '\n",
      "  279  SELECT MIN(start_date), MAX(end_date)\n",
      "  280  FROM `bike_trip_data.bikeshare_trips`; '\n",
      "  281  bq query --use_legacy_sql=false '\n",
      "  282  SELECT SUM(total_bikes)\n",
      "  283  FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  284  FROM (SELECT station_id, docks_available, bikes_available, time as date,\n",
      "  285  (docks_available + bikes_available) as total_bikes\n",
      "  286  FROM `bike_trip_data.bikeshare_status`\n",
      "  287  WHERE DATE(time) = '2016-08-31') AS bikes_on_date) AS distinct_station_bikes_on_date; '\n",
      "  288  bq query --use_legacy_sql=false \"\n",
      "  289   SELECT SUM(total_bikes)\n",
      "  290  >         FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  291  >               FROM (SELECT station_id, docks_available, bikes_available, time as date, \n",
      "  292  >                       (docks_available + bikes_available) as total_bikes\n",
      "  293  >                     FROM `bike_trip_data.bikeshare_status`\n",
      "  294  >                     WHERE DATE(time) = '2016-08-31') AS bikes_on_date) AS distinct_station_bikes_on_date; \"\n",
      "  295  bq query --use_legacy_sql=false \"\n",
      "  296          SELECT SUM(total_bikes)\n",
      "  297          FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  298                FROM (SELECT station_id, docks_available, bikes_available, time as date, \n",
      "  299                        (docks_available + bikes_available) as total_bikes\n",
      "  300                      FROM `bike_trip_data.bikeshare_status`\n",
      "  301                      WHERE DATE(time) = '2016-08-31') AS bikes_on_date) AS distinct_station_bikes_on_date; \"\n",
      "  302  bq query --use_legacy_sql=false '\n",
      "  303  SELECT SUM(total_bikes)\n",
      "  304  FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  305  FROM (SELECT station_id, docks_available, bikes_available, time as date,\n",
      "  306  (docks_available + bikes_available) as total_bikes\n",
      "  307  FROM `bike_trip_data.bikeshare_status`\n",
      "  308  WHERE DATE(time) = '2016-08-31') AS bikes_on_date) AS distinct_station_bikes_on_date; '\n",
      "  309  ls -la\n",
      "  310  cd w205\n",
      "  311  ls\n",
      "  312  cd project-1-jacob-sycoff/\n",
      "  313  git add project-1-jacob-sycoff.ipynb \n",
      "  314  git commit -m \"Working on part two, the fourth question\"\n",
      "  315  git push origin assignment\n",
      "  316  bq query --use_legacy_sql=false '\n",
      "  317  > SELECT *\n",
      "  318  > FROM (SELECT COUNT(*)\n",
      "  319  >       FROM bike_trip_data.bikeshare_trips\n",
      "  320  >       WHERE TIME(start_date) < '12:00:00')\n",
      "  321  >       AS num_morning '\n",
      "  322  bq query --use_legacy_sql=false '\n",
      "  323  SELECT *\n",
      "  324  FROM (SELECT COUNT(*)\n",
      "  325  FROM bike_trip_data.bikeshare_trips\n",
      "  326  WHERE TIME(start_date) < \"12:00:00\")\n",
      "  327  AS num_morning '\n",
      "  328  bq query --use_lagacy_sql=false '\n",
      "  329          SELECT SUM(total_bikes)\n",
      "  330          FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  331                FROM (SELECT station_id, docks_available, bikes_available, time as date, \n",
      "  332                        (docks_available + bikes_available) as total_bikes\n",
      "  333                      FROM `bike_trip_data.bikeshare_status`\n",
      "  334                      WHERE DATE(time) = \"2016-08-31\") AS bikes_on_date) AS distinct_station_bikes_on_date;\n",
      "  335  '\n",
      "  336  bq query --use_legacy_sql=false '\n",
      "  337          SELECT SUM(total_bikes)\n",
      "  338          FROM (SELECT DISTINCT station_id, total_bikes\n",
      "  339                FROM (SELECT station_id, docks_available, bikes_available, time as date, \n",
      "  340                        (docks_available + bikes_available) as total_bikes\n",
      "  341                      FROM `bike_trip_data.bikeshare_status`\n",
      "  342                      WHERE DATE(time) = \"2016-08-31\") AS bikes_on_date) AS distinct_station_bikes_on_date; '\n",
      "  343  git add project-1-jacob-sycoff.ipynb \n",
      "  344  git commit -m  \" finished with first 4 questions of part 2\"\n",
      "  345  git push origin assignment\n",
      "  346  sudo chown -R jupyter:jupyter ~/w205\n",
      "  347  cd ~/w205/course-content\n",
      "  348  git pull --all\n",
      "  349  cd\n",
      "  350  docker ps -a\n",
      "  351  docker network ls\n",
      "  352  docker pull midsw205/base:latest\n",
      "  353  docker pull midsw205/base:0.1.8\n",
      "  354  docker pull midsw205/base:0.1.9\n",
      "  355  docker pull redis\n",
      "  356  docker pull confluentinc/cp-zookeeper:latest\n",
      "  357  docker pull confluentinc/cp-kafka:latest\n",
      "  358  docker pull midsw205/spark-python:0.0.5\n",
      "  359  docker pull midsw205/spark-python:0.0.6\n",
      "  360  docker pull midsw205/cdh-minimal:latest\n",
      "  361  docker pull midsw205/hadoop:0.0.2\n",
      "  362  docker pull midsw205/presto:0.0.1\n",
      "  363  ls\n",
      "  364  clear\n",
      "  365  mkdir ~/w205/kafka\n",
      "  366  cd ~/w205/kafka\n",
      "  367  cp ~/w205/course-content/06-Transforming-Data/docker-compose.yml ~/w205/kafka/\n",
      "  368  docker-compose up -d\n",
      "  369  docker-compose ps\n",
      "  370  docker-compose logs zookeeper | grep -i binding\n",
      "  371  docker-compose logs kafka | grep -i started\n",
      "  372  docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  373  ls\n",
      "  374  docker ps -a\n",
      "  375  docker-compose ps\n",
      "  376  docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181\n",
      "  377  docker-compose exec kafka bash -c \"seq 42 | kafka-console-producer --request-required-acks 1 --broker-list localhost:29092 --topic foo && echo 'Produced 42 messages.'\"\n",
      "  378  docker-compose exec kafka kafka-console-consumer --bootstrap-server localhost:29092 --topic foo --from-beginning --max-messages 42\n",
      "  379  docker-compose down\n",
      "  380  docker-compose ps\n",
      "  381  docker ps -a\n",
      "  382  curl -L -o github-example-large.json https://goo.gl/Y4MD58\n",
      "  383  docker-compose up -d\n",
      "  384  docker-compose logs -f kafka\n",
      "  385  docker-compose down\n",
      "  386  docker-compose ps\n",
      "  387  docker ps -a\n",
      "  388  docker compuse up -d\n",
      "  389  curl -L -o github-example-large.json https://goo.gl/Y4MD58\n",
      "  390  docker-compose up -d\n",
      "  391  docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  392  docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181\n",
      "  393  docker-compose exec mids bash -c \"cat /w205/kafka/github-example-large.json\"\n",
      "  394  docker-compose exec mids bash -c \"cat /w205/kafka/github-example-large.json | jq '.'\"\n",
      "  395  docker-compose exec mids bash -c \"cat /w205/kafka/github-example-large.json | jq '.[]' -c\"\n",
      "  396  docker-compose exec mids bash -c \"cat /w205/kafka/github-example-large.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\"\n",
      "  397  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t foo -o beginning -e\"\n",
      "  398  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t foo -o beginning -e\" | wc -l\n",
      "  399  docker-compose down\n",
      "  400  docker-compose ls\n",
      "  401  docker-compose ps\n",
      "  402  docker ps -a\n",
      "  403  cd\n",
      "  404  ls\n",
      "  405  cd w205\n",
      "  406  ls\n",
      "  407  git clone https://github.com/mids-w205-crook/project-2-jacob-sycoff.git\n",
      "  408  ls\n",
      "  409  cd project-2-jacob-sycoff/\n",
      "  410  ls\n",
      "  411  git branch assignment\n",
      "  412  git checkout assignment\n",
      "  413  git status\n",
      "  414  ls\n",
      "  415  curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
      "  416  ls -lh\n",
      "  417  ls -la\n",
      "  418  ls -lh\n",
      "  419  cat assessment-attempts-20180128-121051-nested.json \n",
      "  420  ls -lh\n",
      "  421  docker compose down\n",
      "  422  docker-compose down\n",
      "  423  docker-compose ps\n",
      "  424  ls\n",
      "  425  clear\n",
      "  426  ls\n",
      "  427  cd w205/kafka\n",
      "  428  ls\n",
      "  429  docker-compose logs -f kafka\n",
      "  430  cd \n",
      "  431  cd w205\n",
      "  432  clear\n",
      "  433  bq query --use_legacy_sql=false '\n",
      "  434  SELECT morning_trips/total_days as avg_morning_trips_per_day, afternoon_trips/total_days as avg_afternoon_trips_per_day\n",
      "  435  FROM (SELECT COUNT(*) as morning_trips\n",
      "  436        FROM bike_trip_data.bikeshare_trips\n",
      "  437        WHERE (TIME(start_date) < \"12:00:00\")),\n",
      "  438        \n",
      "  439        (SELECT COUNT(*) as afternoon_trips\n",
      "  440        FROM bike_trip_data.bikeshare_trips\n",
      "  441        WHERE (TIME(start_date) >= \"12:00:00\")),      \n",
      "  442        \n",
      "  443        (SELECT COUNT(*) AS total_days\n",
      "  444         FROM (SELECT DISTINCT DATE(start_date)\n",
      "  445               FROM bike_trip_data.bikeshare_trips))\n",
      "  446         \n",
      "  447  '\n",
      "  448  ls\n",
      "  449  cd project-1-jacob-sycoff/\n",
      "  450  git add project-1-jacob-sycoff.ipynb \n",
      "  451  git commit -m \"done with part 1 of section 2\"\n",
      "  452  git push origin assignment\n",
      "  453  ls\n",
      "  454  cd w205\n",
      "  455  cd project-1-jacob-sycoff/\n",
      "  456  git add project-1-jacob-sycoff.ipynb \n",
      "  457  git commit -m \"Working on last questions of section 2\"\n",
      "  458  git push origin assignment\n",
      "  459  bq query --use_legacy_sql=false '\n",
      "  460              SELECT start_station_name, COUNT(start_station_name) as num_trips_begun_here_in_2015\n",
      "  461              FROM (SELECT *\n",
      "  462                    FROM bike_trip_data.better_trips\n",
      "  463                    WHERE (year = 2015))\n",
      "  464              GROUP BY start_station_name\n",
      "  465              ORDER BY num_trips_begun_here_in_2015 DESC\n",
      "  466              LIMIT 10; '\n",
      "  467  bq query --use_legacy_sql=false '\n",
      "  468  +-----------------------------------------------+------------------------------+\n",
      "  469  |              start_station_name               | num_trips_begun_here_in_2015 |\n",
      "  470  +-----------------------------------------------+------------------------------+\n",
      "  471  | San Francisco Caltrain (Townsend at 4th)      |                        24827 |\n",
      "  472  | San Francisco Caltrain 2 (330 Townsend)       |                        22274 |\n",
      "  473  | Harry Bridges Plaza (Ferry Building)          |                        17344 |\n",
      "  474  | Temporary Transbay Terminal (Howard at Beale) |                        14668 |\n",
      "  475  | Steuart at Market                             |                        14577 |\n",
      "  476  | Embarcadero at Sansome                        |                        14352 |\n",
      "  477  | 2nd at Townsend                               |                        13762 |\n",
      "  478  | Townsend at 7th                               |                        13695 |\n",
      "  479  | Market at Sansome                             |                        11301 |\n",
      "  480  | Market at 10th                                |                        11049 |\n",
      "  481  '\n",
      "  482  bq query --use_legacy_sql=false '\n",
      "  483              SELECT end_station_name, COUNT(end_station_name) as num_trips_ended_here_in_2015\n",
      "  484              FROM (SELECT *\n",
      "  485                    FROM bike_trip_data.better_trips\n",
      "  486                    WHERE (year = 2015))\n",
      "  487              GROUP BY start_station_name\n",
      "  488              ORDER BY num_trips_ended_here_in_2015 DESC\n",
      "  489              LIMIT 10; '\n",
      "  490  bq query --use_legacy_sql=false '\n",
      "  491              SELECT start_station_name, end_station_name, COUNT(trip_name) AS num_trips_in_2015,\n",
      "  492              FROM (SELECT *, CONCAT(start_station_name, end_station_name) as trip_name\n",
      "  493                    FROM bike_trip_data.better_trips\n",
      "  494                    WHERE (year = 2015))\n",
      "  495              GROUP BY trip_name, start_station_name, end_station_name\n",
      "  496              ORDER BY num_trips_in_2015 DESC\n",
      "  497              LIMIT 10; '\n",
      "  498  bq query --use_legacy_sql=false '\n",
      "  499              SELECT start_station_name, end_station_name, COUNT(trip_name) AS num_trips_in_2015,\n",
      "  500              FROM (SELECT *, CONCAT(start_station_name, end_station_name) as trip_name\n",
      "  501                    FROM bike_trip_data.better_trips)\n",
      "  502              GROUP BY trip_name, start_station_name, end_station_name\n",
      "  503              ORDER BY num_trips_in_2015 DESC\n",
      "  504              LIMIT 10; '\n",
      "  505  git add project-1-jacob-sycoff.ipynb \n",
      "  506  git commit -m \"almost done w p2\"\n",
      "  507  git push origin assignment\n",
      "  508  bq query --use_legacy_sql=false '\n",
      "  509              SELECT start_station_name, end_station_name, COUNT(trip_name) AS num_trips,\n",
      "  510              FROM (SELECT *, CONCAT(start_station_name, end_station_name) as trip_name\n",
      "  511                    FROM bike_trip_data.better_trips)\n",
      "  512              WHERE start_station_name = end_station_name\n",
      "  513              GROUP BY trip_name, start_station_name, end_station_name\n",
      "  514              ORDER BY num_trips DESC\n",
      "  515              LIMIT 10; '\n",
      "  516  pip install google-cloud-bigquery-storage\n",
      "  517  git add project-1-jacob-sycoff.ipynb \n",
      "  518  git commit -m \"part 2 done. Recently began part 3\"\n",
      "  519  git push origin assignment\n",
      "  520  cd w205\n",
      "  521  cd project-1-jacob-sycoff/\n",
      "  522  git add project-1-jacob-sycoff.ipynb \n",
      "  523  git commit -m ' First Question of part 3 is done!!\"\n",
      "  524  git add project-1-jacob-sycoff.ipynb \n",
      "  525  git commit -m ' Finished first question of part 3'\n",
      "  526  git push origin assignment\n",
      "  527  ls\n",
      "  528  git add project-1-jacob-sycoff.ipynb \n",
      "  529  git commit -m \" coming up with offers for part 3 \"\n",
      "  530  git push origin assignment\n",
      "  531  cd w203\n",
      "  532  ls\n",
      "  533  cd w205\n",
      "  534  ls\n",
      "  535  cd project-1-jacob-sycoff/\n",
      "  536  git add project-1-jacob-sycoff.ipynb \n",
      "  537  git commit -m \"almost done\"\n",
      "  538  git push origin assignment\n",
      "  539  git add p\n",
      "  540  git add project-1-jacob-sycoff.ipynb \n",
      "  541  git commit -m \"project done\"\n",
      "  542  git push origin assignment\n",
      "  543  git checkout master\n",
      "  544  git commit -m \"project done\"\n",
      "  545  git add project-1-jacob-sycoff.ipynb \n",
      "  546  git commit -m \"project done\"\n",
      "  547  git push\n",
      "  548  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  549  docker pull midsw205/base:latest\n",
      "  550  docker pull midsw205/base:0.1.8\n",
      "  551  docker pull midsw205/base:0.1.9\n",
      "  552  docker pull redis\n",
      "  553  docker pull confluentinc/cp-zookeeper:latest\n",
      "  554  docker pull confluentinc/cp-kafka:latest\n",
      "  555  docker pull midsw205/spark-python:0.0.5\n",
      "  556  docker pull midsw205/spark-python:0.0.6\n",
      "  557  docker pull midsw205/cdh-minimal:latest\n",
      "  558  docker pull midsw205/hadoop:0.0.2\n",
      "  559  docker pull midsw205/presto:0.0.1\n",
      "  560  docker-compose exec mids bash -c \"cat /w205/kafka/github-example-large.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\"\n",
      "  561  docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  562  cd ~/w205/spark-with-kafka\n",
      "  563  docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  564  docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181\n",
      "  565  docker-compose exec kafka bash -c \"seq 42 | kafka-console-producer --request-required-acks 1 --broker-list kafka:29092 --topic foo && echo 'Produced 42 messages.'\"\n",
      "  566  docker-compose exec spark pyspark\n",
      "  567  docker-compose down\n",
      "  568  docker-compose -ps\n",
      "  569  cd ~/w205\n",
      "  570  curl -L -o github-example-large.json https://goo.gl/Y4MD58\n",
      "  571  cd ~/w205/spark-with-kafka\n",
      "  572  docker-compose up -d\n",
      "  573  docker-compose logs -f kafka\n",
      "  574  cp ~/w205/course-content/06-Transforming-Data/docker-compose.yml ~/w205/project-2-jacob-sycoff/\n",
      "  575  docker-compose up -d\n",
      "  576  cd /w205/project-2-jacob-sycoff\n",
      "  577  cd w205\n",
      "  578  ls\n",
      "  579  cd project-2-jacob-sycoff/\n",
      "  580  cp ~/w205/course-content/06-Transforming-Data/docker-compose.yml ~/w205/project-2-jacob-sycoff/\n",
      "  581  docker-compose up -d\n",
      "  582  docker-compose ps\n",
      "  583  docker ps -a\n",
      "  584  docker-compose logs -f kafka\n",
      "  585  Page down\n",
      "  586  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  587  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
      "  588  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"\n",
      "  589  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\"\n",
      "  590  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
      "  591  docker-compose down\n",
      "  592  mkdir ~/w205/spark-with-kafka\n",
      "  593  cd ~/w205/spark-with-kafka\n",
      "  594  cp ~/w205/course-content/07-Sourcing-Data/docker-compose.yml .\n",
      "  595  docker-compose up -d\n",
      "  596  docker-compose logs -f kafka\n",
      "  597  cd ..\n",
      "  598  docker ps -a\n",
      "  599  cd spark-with-kafka\n",
      "  600  docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  601  docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181\n",
      "  602  docker-compose exec mids bash -c \"cat /w205/github-example-large.json\"\n",
      "  603  docker-compose exec mids bash -c \"cat /w205/github-example-large.json | jq '.'\"\n",
      "  604  docker-compose exec mids bash -c \"cat /w205/github-example-large.json | jq '.[]' -c\"\n",
      "  605  docker-compose exec mids bash -c \"cat /w205/github-example-large.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\"\n",
      "  606  docker-compose exec spark pyspark\n",
      "  607  docker-compose down\n",
      "  608  ls -l\n",
      "  609  docker-compose down\n",
      "  610  docker-compose ps\n",
      "  611  cd ..\n",
      "  612  cd project-2-jacob-sycoff/\n",
      "  613  docker-compose down\n",
      "  614  ls -la\n",
      "  615  cd w205\n",
      "  616  ls -la\n",
      "  617  git clone https://github.com/kevin-crook-ucb/ucb_w205_crook_supplement.git\n",
      "  618  ls -la\n",
      "  619  cd ucb_w205_crook_supplement/\n",
      "  620  ls -la\n",
      "  621  cd 2020_fall\n",
      "  622  ls\n",
      "  623  cd 2020_Fall/\n",
      "  624  ls\n",
      "  625  docker-compose ps\n",
      "  626  docker ps -a\n",
      "  627  cd ..\n",
      "  628  ls -lh\n",
      "  629  ls\n",
      "  630  cd w205\n",
      "  631  ls\n",
      "  632  cd project-2-jacob-sycoff/\n",
      "  633  git status\n",
      "  634  ls -l\n",
      "  635  cp ~/w205/course-content/07-Sourcing-Data/docker-compose.yml ~/w205/project-2-jacob-sycoff/\n",
      "  636  ls -lh\n",
      "  637  xdg-open docker-compose.yml\n",
      "  638  open docker-compose.yml\n",
      "  639  nano docker-compose.yml\n",
      "  640  docker-compose up -d\n",
      "  641  docker-compose ps\n",
      "  642  docker ps -a\n",
      "  643  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  644  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
      "  645  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"\n",
      "  646  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\n",
      "  647  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t foo && echo 'Produced 100 messages.'\"\n",
      "  648  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
      "  649  docker-compose down\n",
      "  650  docker-compose up -d\n",
      "  651  docker-compose exec spark bash\n",
      "  652  jupyter@tensorflow-2-3-20200827-182119:~/w205/project-2-jacob-sycoff$ \n",
      "  653  docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
      "  654  mkdir ~/w205/spark-with-kafka-and-hdfs\n",
      "  655  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  656  cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .\n",
      "  657  cd ~/w205\n",
      "  658  curl -L -o players.json https://goo.gl/vsuCpZ\n",
      "  659  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  660  docker-compose up -d\n",
      "  661  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
      "  662  docker-compose exec kafka kafka-topics --create --topic players --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  663  docker-compose exec mids bash -c \"cat /w205/players.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t players\"\n",
      "  664  docker-compose exec spark pyspark\n",
      "  665  docker-compose up -d\n",
      "  666  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  667  docker-compose up -d\n",
      "  668  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
      "  669  docker-compose exec kafka kafka-topics --create --topic players --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  670  docker-compose exec mids bash -c \"cat /w205/players.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t players\"\n",
      "  671  docker-compose exec spark pyspark\n",
      "  672  cd ~/w205\n",
      "  673  curl -L -o players.json https://goo.gl/vsuCpZ\n",
      "  674  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  675  docker-compose up -d\n",
      "  676  docker-compose logs -f kafka\n",
      "  677  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
      "  678  docker-compose up -d\n",
      "  679  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  680  docker-compose up -d\n",
      "  681  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
      "  682  docker-compose exec kafka kafka-topics --create --topic players --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  683  docker-compose exec mids bash -c \"cat /w205/players.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t players\"\n",
      "  684  docker-compose exec spark pyspark\n",
      "  685  cd ..\n",
      "  686  ls -la\n",
      "  687  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  688  cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .\n",
      "  689  cd ~/w205\n",
      "  690  curl -L -o players.json https://goo.gl/vsuCpZ\n",
      "  691  cd ~/w205/spark-with-kafka-and-hdfs\n",
      "  692  docker-compose up -d\n",
      "  693  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
      "  694  docker-compose exec kafka kafka-topics --create --topic players --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  695  docker-compose exec mids bash -c \"cat /w205/players.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t players\"\n",
      "  696  docker-compose exec spark pyspark\n",
      "  697  docker-compose down\n",
      "  698  docker-compose ps -a\n",
      "  699  docker-compose ps\n",
      "  700  docker ps -a\n",
      "  701  docker-compose down\n",
      "  702  cd ..\n",
      "  703  docker-compose down\n",
      "  704  docker ps -a\n",
      "  705  cd project-2\n",
      "  706  cd project-2-jacob-sycoff/\n",
      "  707  ls -la\n",
      "  708  docker-compose down\n",
      "  709  docker-compose up -d\n",
      "  710  docker-compose logs -f kafka\n",
      "  711  ls\n",
      "  712  cd w205\n",
      "  713  ls\n",
      "  714  cd project-2-jacob-sycoff/\n",
      "  715  ls\n",
      "  716  cd ..\n",
      "  717  ls\n",
      "  718  cd w205\n",
      "  719  ls\n",
      "  720  cd project-2-jacob-sycoff/\n",
      "  721  git pull\n",
      "  722  git status\n",
      "  723  ls\n",
      "  724  nano docker-compose.yml \n",
      "  725  git status\n",
      "  726  ls\n",
      "  727  cd ..\n",
      "  728  ls\n",
      "  729  cd course_\n",
      "  730  cd course-content/\n",
      "  731  ls\n",
      "  732  cd 07-Sourcing-Data/\n",
      "  733  ls\n",
      "  734  cd ..\n",
      "  735  cd 08-Querying-Data/\n",
      "  736  ls\n",
      "  737  cd .\n",
      "  738  cd ..\n",
      "  739  ls\n",
      "  740  cd ..\n",
      "  741  ls\n",
      "  742  cd project-2-jacob-sycoff/\n",
      "  743  ls\n",
      "  744  curl -L -o github-example-large.json https://goo.gl/Y4MD58\n",
      "  745  ls\n",
      "  746  cp ~/w205/course-content/08-Querying-Data/docker-compose.yml ~/w205/project-2-jacob-sycoff/\n",
      "  747  nano docker-compose.yml \n",
      "  748  cat docker-compose.yml \n",
      "  749  ls\n",
      "  750  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  751  curl -L -o github-example-large.json https://goo.gl/Y4MD58\n",
      "  752  docker-compose up -d\n",
      "  753  docker-compose ps\n",
      "  754  docker ps -a\n",
      "  755  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  756  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
      "  757  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced 100 messages.'\"\n",
      "  758  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
      "  759  docker-compose exec spark bash\n",
      "  760  docker-compose exec spark pyspark\n",
      "  761  docker-compose down\n",
      "  762  docker-compose up -d\n",
      "  763  docker-compose exec spark bash\n",
      "  764  docker-compose exec spark pyspark\n",
      "  765  docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
      "  766  history > <jupyter>-history.txthistory > <jupyter>-history.txt\n",
      "  767  history > <jupyter>-history.txt\n",
      "  768  docker-compose down\n",
      "  769  history > <jupyter>-history.txt\n",
      "  770  ls\n",
      "  771  cd w205\n",
      "  772  ls\n",
      "  773  cd project-2-jacob-sycoff/\n",
      "  774  ls\n",
      "  775  history > <jupyter>-history.txt\n",
      "  776  history > jupyter-history.txt\n",
      "  777  docker-compose down\n",
      "  778  docker-compose ps\n",
      "  779  focker ps -a\n",
      "  780  docker ps -a\n",
      "  781  cd w205\n",
      "  782  ls\n",
      "  783  cd project-2-jacob-sycoff/\n",
      "  784  ls\n",
      "  785  cd ..\n",
      "  786  telnet google.com 80\n",
      "  787  sudo apt-get install telnet\n",
      "  788  telnet google.com 80\n",
      "  789  cd temp\n",
      "  790  cd ~/temp\n",
      "  791  ls\n",
      "  792  mkdir temp\n",
      "  793  cd temp\n",
      "  794  ls\n",
      "  795  telnet google.com 80\n",
      "  796  openssl s_client -connect google.com:443\n",
      "  797  openssl s_client -connect api.wheretheiss.at:443\n",
      "  798  echo | openssl s_client -connect google.com:443 2>&1 | sed --quiet '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > google.com.1.crt\n",
      "  799  cat google.com.1.crt\n",
      "  800  openssl x509 -in google.com.1.crt -noout -text\n",
      "  801  echo | openssl s_client -connect google.com:443 -showcerts 2>&1 | sed --quiet '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > google.com.2.crt\n",
      "  802  cat google.com.2.crt\n",
      "  803  openssl x509 -in google.com.2.crt -noout -text\n",
      "  804  echo | openssl s_client -connect api.wheretheiss.at:443 -showcerts -servername api.wheretheiss.at 2>&1 | sed --quiet '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > api.wheretheiss.at.crt\n",
      "  805  cat api.wheretheiss.at.crt\n",
      "  806  openssl x509 -in api.wheretheiss.at.crt -noout -text\n",
      "  807  clear\n",
      "  808  cd ../w205\n",
      "  809  ls\n",
      "  810  cd flask-with-kafka/\n",
      "  811  docker-compose exec mids curl http://localhost:5000/\n",
      "  812  docker-compose exec mids curl http://localhost:5000/purchase_a_sword\n",
      "  813  docker-compose exec mids curl http://localhost:5000/\n",
      "  814  docker-compose exec mids curl http://localhost:5000/purchase_a_sword\n",
      "  815  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t events -o beginning -e\"\n",
      "  816  docker-compose exec mids curl http://localhost:5000/\n",
      "  817  docker-compose exec mids curl http://localhost:5000/purchase_a_sword\n",
      "  818  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t events -o beginning -e\"\n",
      "  819  ls\n",
      "  820  cd w205\n",
      "  821  ls\n",
      "  822  cd project-\n",
      "  823  cd project-2-jacob-sycoff/\n",
      "  824  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  825  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
      "  826  docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced 100 messages.'\"\n",
      "  827  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
      "  828  cd w20\n",
      "  829  cd w205/\n",
      "  830  ls\n",
      "  831  cd project-2-jacob-sycoff/\n",
      "  832  ls\n",
      "  833  vim docker-compose.yml\n",
      "  834  curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
      "  835  curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
      "  836  docker-compose up -d\n",
      "  837  docker-compose ps\n",
      "  838  docker-compose exec spark bash\n",
      "  839  docker-compose exec spark pyspark\n",
      "  840  docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
      "  841  ls\n",
      "  842  cd w205\n",
      "  843  ls\n",
      "  844  cd project-2-jacob-sycoff/\n",
      "  845  ls\n",
      "  846  git status\n",
      "  847  git checkout assignment\n",
      "  848  git add jupyter-history.txt\n",
      "  849  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  850  git add docker-compose.yml\n",
      "  851  git commit -m \"adding all files\"\n",
      "  852  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
      "  853  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  854  git commit -m \"startup process explained\"\n",
      "  855  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  856  git commit -m \"set up analysis section\"\n",
      "  857  cd w205\n",
      "  858  ls\n",
      "  859  cd project-2-jacob-sycoff/\n",
      "  860  ls\n",
      "  861  docker-compose ps\n",
      "  862  docker ps -a\n",
      "  863  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  864  git commit -m \"sample question 1 and 2 answered\"\n",
      "  865  docker-compose exec mids curl http://localhost:5000/\n",
      "  866  cd ~/w205/flask-with-kafka-and-spark/\n",
      "  867  ls\n",
      "  868  nano docker-compose.yml\n",
      "  869  docker-compose exec mids curl http://localhost:5000/\n",
      "  870  docker-compose exec mids curl http://localhost:5000/purchase_a_sword\n",
      "  871  docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning -e\n",
      "  872  docker-compose exec mids curl http://localhost:5000/\n",
      "  873  docker-compose exec mids curl http://localhost:5000/purchase_a_sword\n",
      "  874  docker-compose exec mids curl http://localhost:5000/purchase_a_frog\n",
      "  875  docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning -e\n",
      "  876  docker-compose exec spark pyspark\n",
      "  877  docker-compose down\n",
      "  878  ls\n",
      "  879  docker ps\n",
      "  880  docker ps -a\n",
      "  881  ls\n",
      "  882  docker ps -a\n",
      "  883  docker-compose ps\n",
      "  884  cd w205\n",
      "  885  cs project-2-jacob-sycoff/\n",
      "  886  docker-compose ps\n",
      "  887  cd project-2-jacob-sycoff/\n",
      "  888  docker-compose ps\n",
      "  889  docker ps -a\n",
      "  890  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  891  git commit -m \"sample questions done\"\n",
      "  892  ls\n",
      "  893  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  894  git commit -m \"working on my question 1\"\n",
      "  895  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  896  git commit -m \"my questions 1 and 2 done\"\n",
      "  897  docker-compose down\n",
      "  898  mkdir ~/w205/python-requests/\n",
      "  899  cd ~/w205/python-requests/\n",
      "  900  curl https://raw.githubusercontent.com/kevin-crook-ucb/ucb_w205_crook_supplement/master/2020_Fall/synch_10_python_requests.ipynb --output python_requests.ipynb\n",
      "  901  mkdir ~/w205/flask-with-kafka-and-spark/\n",
      "  902  cd ~/w205/flask-with-kafka-and-spark/\n",
      "  903  cp ~/w205/course-content/10-Transforming-Streaming-Data/docker-compose.yml .\n",
      "  904  docker-compose up -d\n",
      "  905  docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
      "  906  cp ~/w205/course-content/10-Transforming-Streaming-Data/game_api_with_json_events.py .\n",
      "  907  docker-compose exec mids env FLASK_APP=/w205/flask-with-kafka-and-spark/game_api_with_json_events.py flask run --host 0.0.0.0\n",
      "  908  cd ~/w205/flask-with-kafka-and-spark/\n",
      "  909  cp ~/w205/course-content/10-Transforming-Streaming-Data/docker-compose.yml .\n",
      "  910  docker-compose down\n",
      "  911  docker-compose up -d\n",
      "  912  docker-compose exec mids env FLASK_APP=/w205/flask-with-kafka-and-spark/game_api_with_extended_json_events.py flask run --host 0.0.0.0\n",
      "  913  docker-compose exec mids env FLASK_APP=/w205/flask-with-kafka-and-spark/game_api_with_json_events.py flask run --host 0.0.0.0\n",
      "  914  cp ~/w205/course-content/10-Transforming-Streaming-Data/game_api_with_extended_json_events.py .\n",
      "  915  docker-compose exec mids env FLASK_APP=/w205/flask-with-kafka-and-spark/game_api_with_extended_json_events.py flask run --host 0.0.0.0\n",
      "  916  cd w205\n",
      "  917  cd project-2-jacob-sycoff/\n",
      "  918  git add Jacob_Sycoff_w205_project_2.ipynb \n",
      "  919  git commit -m \"almost done\"\n",
      "  920  history > jupyter-history.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = open('jupyter-history.txt')\n",
    "print(history.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set up process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) command to bring up the cluster\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "###### a) check open docker-compose containers\n",
    "```bash\n",
    "    docker-compose ps\n",
    "```\n",
    "###### b) check open docker containers\n",
    "```bash\n",
    "    docker ps -a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) command to create the kafka topic with a meaningful name (most students choose assessments)\n",
    "```bash\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) command to publish the assessments json data to the kafka topic using kafkacat\n",
    "```bash\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-jacob-sycoff/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Published assessments json data to kafka topic using kafkacat.'\"\n",
    "\n",
    "docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) command to shutdown the cluster\n",
    "\n",
    "```spark\n",
    "exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing up jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Enter spark bash:\n",
    "```bash\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "#####  a) In spark bash, link w205 with w205:\n",
    "```spark\n",
    "    ln -s /w205 w205\n",
    "```\n",
    "##### b) Exit spark bash:\n",
    "```spark\n",
    "exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Create Jupyter environment:\n",
    "```bash\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Change 0.0.0.0 in resulting link to external IP address in google cloud, then paste into an incognito Google Chrome window to minimize chances of errors. Jupyter notebook will come up and pyspark will work within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Begins Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) How many assessments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3280|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_assessments = spark.sql(\"select count(*) from (select sequences_id from sequences)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3280 assessments in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) What's the name of your Kafka topic? How did you come up with that name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of my Kafka topic is assessments because of the contents of the information within the dataset - infomration from a service that delivers assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) How many people took Learning Git?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|394  |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count from (select exam_name, count(*) as count from(select exam_name from assessments) group by exam_name) where exam_name='Learning Git' \").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "394 people took \"Learning Git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) What is the least common course taken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|exam_name                                        |\n",
      "+-------------------------------------------------+\n",
      "|Native Web Apps for Android                      |\n",
      "|Learning to Visualize Data with D3.js            |\n",
      "|Nulls, Three-valued Logic and Missing Information|\n",
      "|Operating Red Hat Enterprise Linux Servers       |\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name from(select exam_name, count(*) as count from(select exam_name from assessments) group by exam_name order by count asc) limit 4 \").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four above courses all are the least common with only 1 participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5? What is the most common course taken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|exam_name   |\n",
      "+------------+\n",
      "|Learning Git|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name from(select exam_name, count(*) as count from(select exam_name from assessments) group by exam_name order by count desc) limit 1\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Has the number of tests taken increased over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|year_month|count(1)|\n",
      "+----------+--------+\n",
      "|2017-11   |521     |\n",
      "|2017-12   |1516    |\n",
      "|2018-01   |1243    |\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select year_month, count(*) from(select started_at, substring(started_at, 0, 7)as year_month from assessments order by year_month asc) group by year_month\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests taken over time increased and then decreased, but the information is not useful because there is only data present from 3 months. It is therefore my reccomendation to collect and store data consistently for every test given every month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Are there more certification or non-certification exams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|certification|count|\n",
      "+-------------+-----+\n",
      "|null         |132  |\n",
      "|false        |3148 |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select certification, count(*) as count from(select certification from assessments) group by certification\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only value given for certification is false, while a small subset have a null value for certification.  This bring into question the reason for the collection of this data. Should the company remove this field? Does the company have a potential opportunity to bringing in business for certification exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) What percent of tests were left at least 25% unanswered or incomplete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I create the function that will allow me to create a table with columns showing unanswered and total questions per test, apply it, and analyze the resulting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_unanswered_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"unanswered\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"] and \"incomplete\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                                    \n",
    "                my_dict = {\"unanswered\": raw_dict[\"sequences\"][\"counts\"][\"unanswered\"], \n",
    "                           \"incomplete\": raw_dict[\"sequences\"][\"counts\"][\"incomplete\"],\n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_unanswered_total = assessments.rdd.flatMap(my_lambda_unanswered_total).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_unanswered_total.registerTempTable('ut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|pct_25_or_more_pct_not_done|\n",
      "+---------------------------+\n",
      "|        0.27541984732824426|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*)/(select count(*) from ut) as pct_25_or_more_pct_not_done from (select pct_not_done from (select *, unanswered_incomplete/total as pct_not_done from(select unanswered, incomplete,unanswered + incomplete as unanswered_incomplete, total from ut)) where pct_not_done >=.25)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_unanswered_total.write.mode('overwrite').parquet(\"/tmp/my_unanswered_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ut = spark.read.parquet(\"/tmp/my_unanswered_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.createOrReplaceTempView(\"ut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|pct_25_or_more_pct_not_done|\n",
      "+---------------------------+\n",
      "|        0.27541984732824426|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ut = spark.sql(\"select count(*)/(select count(*) from ut) as pct_25_or_more_pct_not_done from (select pct_not_done from (select *, unanswered_incomplete/total as pct_not_done from(select unanswered, incomplete,unanswered + incomplete as unanswered_incomplete, total from ut)) where pct_not_done >=.25)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 28% of tests have at least 25% of their questions left incomplete or unanswered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
